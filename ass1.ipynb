{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "#question1\n",
    "from numpy import mean\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=2404)\n",
    "\n",
    "print(len(y_test))\n",
    "#print(iris)\n",
    "#print(iris.data.shape) #\n",
    "#print(X_train[0])\n",
    "#print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calulate euclidian distance\n",
    "from math import sqrt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def distance(a,b):\n",
    "    return np.sqrt(np.sum((a-b)**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurancy nn  0.9473684210526315\n",
      "error rate  0.052631578947368474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 0, 1, 1, 2, 2, 0, 2, 2, 0, 0, 1, 2, 2, 1, 2, 1, 0, 0, 2,\n",
       "       1, 2, 2, 0, 2, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#question 3\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "#knn 1\n",
    "def nn(X_train,X_test,y_train):\n",
    "\n",
    "    #print(len(y_test))\n",
    "    pred_labels = []\n",
    "    #distances = []\n",
    "    for test_sample in X_test:\n",
    "        distances =  [distance(test_sample, train_sample) for train_sample in X_train]\n",
    "        #stroe all distance for each test\n",
    "        #distances.append(dist)\n",
    "        #find distance of minimum distance\n",
    "        index_min_dist = np.argmin(distances)\n",
    "        #print(index_min_dist)\n",
    "        pred_labels.append(y_train[index_min_dist])\n",
    "        pred_labels_convert_array = np.array(pred_labels)\n",
    "    #print(\"y_test \",y_test)\n",
    "    #print(\"pred_y \",pred_labels_convert_array )\n",
    "    \n",
    "    print(\"accurancy nn \",np.mean(y_test ==pred_labels_convert_array))\n",
    "    print(\"error rate \", 1 - np.mean(y_test ==pred_labels_convert_array))\n",
    "        #print(distances)\n",
    "        \n",
    "    return pred_labels_convert_array\n",
    "nn(X_train,X_test,y_train)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy k=3  0.9473684210526315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 0, 1, 1, 2, 2, 0, 2, 2, 0, 0, 1, 2, 2, 1, 2, 1, 0, 0, 2,\n",
       "       1, 2, 2, 0, 2, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knn 3\n",
    "def knn(X_train, X_test, y_train, k):\n",
    "    pred_labels = []\n",
    "    for test_sample in X_test:\n",
    "        distances = [distance(test_sample, train_sample) for train_sample in X_train]\n",
    "        index_distances_k = np.argsort(distances)[:k] #get the first 3 nearest distances in term of index\n",
    "        #print(\"list of 3 distances \", index_distances_k)\n",
    "        nearest_labels = [y_train[i]  for i in index_distances_k] #get the three labels\n",
    "        #print(\"list of 3 lables \",nearest_labels)\n",
    "        majority_vote = Counter(nearest_labels).most_common(1)[0][0] #get tuple(key, number times appear) and store anly the key.\n",
    "        #print(majority_vote)\n",
    "        pred_labels.append(majority_vote)\n",
    "        pred_labels_convert_array3 = np.array(pred_labels)\n",
    "        \n",
    "\n",
    "\n",
    "    #print(\"pred_labels_convert_array \",pred_labels_convert_array3)\n",
    "    print(\"accuracy k=3 \",np.mean(y_test ==pred_labels_convert_array3))\n",
    "    return pred_labels_convert_array3\n",
    "#nn(X_train,X_test,y_train)\n",
    "knn(X_train,X_test,y_train,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy k=3  0.9473684210526315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 0, 1, 1, 2, 2, 0, 2, 2, 0, 0, 1, 2, 2, 1, 2, 1, 0, 0, 2,\n",
       "       1, 2, 2, 0, 2, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn(X_train,X_test,y_train,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy k=3  0.9473684210526315\n",
      "accurancy nn  0.9473684210526315\n",
      "error rate  0.052631578947368474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(knn(X_train,X_test,y_train,3) == nn(X_train,X_test,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label, epsilon=1e-8):\n",
    "    results = []\n",
    "\n",
    "    # Add the test sample with assumed label to the training set temporarily\n",
    "    extended_samples = np.vstack([train_samples, test_sample])\n",
    "    extended_labels = np.append(train_labels, assumed_label)\n",
    "\n",
    "    # Compute conformity scores for each sample in the extended training set\n",
    "    for i, sample in enumerate(extended_samples):\n",
    "        same_class_dist = float('inf')\n",
    "        diff_class_dist = float('inf')\n",
    "\n",
    "        for j, other_sample in enumerate(extended_samples):\n",
    "            if i == j:  # Skip comparison with itself\n",
    "                continue\n",
    "            \n",
    "            dist = distance(sample, other_sample)\n",
    "\n",
    "            if extended_labels[j] == extended_labels[i]:  # Same class\n",
    "                same_class_dist = min(same_class_dist, dist)\n",
    "            else:  # Different class\n",
    "                diff_class_dist = min(diff_class_dist, dist)\n",
    "\n",
    "        # Handle division by zero\n",
    "        if same_class_dist == 0.0:\n",
    "            same_class_dist += epsilon\n",
    "        \n",
    "        conformity = diff_class_dist / same_class_dist\n",
    "        results.append((sample, extended_labels[i], conformity))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calulate pValue for assumed 0\n",
    "def pValue(test_score_neg, confScore0,key):\n",
    "    list_pValue_with_lable0  = []\n",
    "\n",
    "    rank = 0\n",
    "    for score in confScore0:\n",
    "        if score <= test_score_neg:\n",
    "            rank +=1\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    p_value = rank / len(confScore0)\n",
    "    list_pValue_with_lable0.append((key,p_value))\n",
    "    #print(\"list pValue0\\n\",list_pValue_with_lable0)\n",
    "    return list_pValue_with_lable0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conformity Scores assuming the test sample is +1:\n",
      "Sample: [0 3], Label: 1, Conformity Score: 0.8944271909999159\n",
      "Sample: [2 2], Label: 1, Conformity Score: 1.5811388300841895\n",
      "Sample: [3 3], Label: 1, Conformity Score: 2.5495097567963922\n",
      "Sample: [-1  1], Label: -1, Conformity Score: 1.4142135623730951\n",
      "Sample: [-1 -1], Label: -1, Conformity Score: 0.7071067811865476\n",
      "Sample: [0 1], Label: -1, Conformity Score: 1.0\n",
      "Sample: [0 0], Label: 1, Conformity Score: 0.35355339059327373\n",
      "\n",
      "Conformity Scores assuming the test sample is -1:\n",
      "Sample: [0 3], Label: 1, Conformity Score: 0.8944271909999159\n",
      "Sample: [2 2], Label: 1, Conformity Score: 1.5811388300841895\n",
      "Sample: [3 3], Label: 1, Conformity Score: 2.5495097567963922\n",
      "Sample: [-1  1], Label: -1, Conformity Score: 2.23606797749979\n",
      "Sample: [-1 -1], Label: -1, Conformity Score: 2.91547594742265\n",
      "Sample: [0 1], Label: -1, Conformity Score: 2.0\n",
      "Sample: [0 0], Label: -1, Conformity Score: 2.8284271247461903\n",
      "0.35355339059327373\n",
      "2.8284271247461903\n",
      "p_vlaure for assumed +1 0.143\n",
      "p_vlaure for assumed -1 0.857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "def compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label, epsilon=1e-8):\n",
    "    results = []\n",
    "\n",
    "    # Add the test sample with assumed label to the training set temporarily\n",
    "    extended_samples = np.vstack([train_samples, test_sample])\n",
    "    extended_labels = np.append(train_labels, assumed_label)\n",
    "\n",
    "    # Compute conformity scores for each sample in the extended training set\n",
    "    for i, sample in enumerate(extended_samples):\n",
    "        same_class_dist = float('inf')\n",
    "        diff_class_dist = float('inf')\n",
    "\n",
    "        for j, other_sample in enumerate(extended_samples):\n",
    "            if i == j:  # Skip comparison with itself\n",
    "                continue\n",
    "            \n",
    "            dist = euclidean_distance(sample, other_sample)\n",
    "\n",
    "            if extended_labels[j] == extended_labels[i]:  # Same class\n",
    "                same_class_dist = min(same_class_dist, dist)\n",
    "            else:  # Different class\n",
    "                diff_class_dist = min(diff_class_dist, dist)\n",
    "\n",
    "        # Handle division by zero\n",
    "        if same_class_dist == 0.0:\n",
    "            same_class_dist += epsilon\n",
    "        \n",
    "        conformity = diff_class_dist / same_class_dist\n",
    "        results.append((sample, extended_labels[i], conformity))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "#retrive the conformity score of assumed label +1\n",
    "def retrieve_conformity_scorePos(train_samples, train_labels, test_sample):\n",
    "    # Compute conformity scores assuming label +1\n",
    "    conformity_scores_pos = compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label=1)\n",
    "    test_score_pos = conformity_scores_pos[-1][2]  # Extract conformity score of the test sample\n",
    "    #print(f\"Conformity Score assuming label +1: {test_score_pos:.3f}\")\n",
    "    return test_score_pos\n",
    "\n",
    "#retrive the conformity score of assumed label -1   \n",
    "def retrieve_conformity_scoreNeg(train_samples, train_labels, test_sample):\n",
    "    # Compute conformity scores assuming label -1\n",
    "    conformity_scores_neg = compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label=-1)\n",
    "    test_score_neg = conformity_scores_neg[-1][2]  # Extract conformity score of the test sample\n",
    "    #print(f\"Conformity Score assuming label -1: {test_score_neg:.3f}\")\n",
    "    return test_score_neg\n",
    "\n",
    "\n",
    "#calulate pValue for assumed -1\n",
    "# Define training data\n",
    "train_samples = np.array([[0, 3], [2, 2], [3, 3], [-1, 1], [-1, -1], [0, 1]])\n",
    "train_labels = np.array([1, 1, 1, -1, -1, -1])  # 1 = positive, -1 = negative\n",
    "#train_samples = X_train\n",
    "#train_labels = y_train\n",
    "# Define test sample\n",
    "test_sample = np.array([0, 0])\n",
    "\n",
    "# Compute conformity scores assuming label +1\n",
    "print(\"Conformity Scores assuming the test sample is +1:\")\n",
    "conformity_scores_zero = compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label=1)\n",
    "for sample, label, score in conformity_scores_zero:\n",
    "    print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score}\")\n",
    "\n",
    "# Compute conformity scores assuming label -1\n",
    "print(\"\\nConformity Scores assuming the test sample is -1:\")\n",
    "conformity_scores_neg = compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label=-1)\n",
    "for sample, label, score in conformity_scores_neg:\n",
    "    print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score}\")\n",
    "\n",
    "\n",
    "\n",
    "confScorePos = retrieve_conformity_scorePos(train_samples, train_labels, test_sample)\n",
    "print(confScorePos)\n",
    "confScoreNeg = retrieve_conformity_scoreNeg(train_samples, train_labels, test_sample)\n",
    "print(f\"{confScoreNeg}\")\n",
    "\n",
    "\n",
    "#calulate pValue for assumed +1\n",
    "def pValuePos(test_score_pos, conformity_scores_pos):\n",
    "    rank = 0\n",
    "    for sample, label, score in conformity_scores_pos:\n",
    "        if score <= test_score_pos:\n",
    "            rank +=1\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    p_value = rank / len(conformity_scores_pos) \n",
    "    \n",
    "    return p_value \n",
    "\n",
    "\n",
    "#calulate pValue for assumed -1\n",
    "def pValueNeg(test_score_neg, conformity_scores_neg):\n",
    "    rank = 0\n",
    "    for sample, label, score in conformity_scores_neg:\n",
    "        if score <= test_score_neg:\n",
    "            rank +=1\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    p_value = rank / len(conformity_scores_neg) \n",
    "    #print(len(conformity_scores_neg))\n",
    "    return p_value \n",
    "\n",
    "\n",
    "#calulate pValue for assumed +1\n",
    "pValueP = pValuePos(confScorePos, conformity_scores_zero)\n",
    "print(f\"p_vlaure for assumed +1 {pValueP:.3f}\")\n",
    "\n",
    "#calulate pValue for assumed -1\n",
    "pValueN = pValueNeg(confScoreNeg, conformity_scores_neg)\n",
    "print(f\"p_vlaure for assumed -1 {pValueN:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conformity Scores assuming the test sample is +1:\n",
    "Sample: [0 3], Label: 1, Conformity Score: 0.8944271909999159\n",
    "Sample: [2 2], Label: 1, Conformity Score: 1.5811388300841895\n",
    "Sample: [3 3], Label: 1, Conformity Score: 2.5495097567963922\n",
    "Sample: [-1  1], Label: -1, Conformity Score: 1.4142135623730951\n",
    "Sample: [-1 -1], Label: -1, Conformity Score: 0.7071067811865476\n",
    "Sample: [0 1], Label: -1, Conformity Score: 1.0\n",
    "Sample: [0 0], Label: 1, Conformity Score: 0.35355339059327373\n",
    "\n",
    "Conformity Scores assuming the test sample is -1:\n",
    "Sample: [0 3], Label: 1, Conformity Score: 0.8944271909999159\n",
    "Sample: [2 2], Label: 1, Conformity Score: 1.5811388300841895\n",
    "Sample: [3 3], Label: 1, Conformity Score: 2.5495097567963922\n",
    "Sample: [-1  1], Label: -1, Conformity Score: 2.23606797749979\n",
    "Sample: [-1 -1], Label: -1, Conformity Score: 2.91547594742265\n",
    "Sample: [0 1], Label: -1, Conformity Score: 2.0\n",
    "Sample: [0 0], Label: -1, Conformity Score: 2.8284271247461903\n",
    "0.35355339059327373\n",
    "2.8284271247461903\n",
    "p_vlaure for assumed +1 0.143\n",
    "p_vlaure for assumed -1 0.857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert_array\n",
      " [[[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.05357142857142857 ]]\n",
      "\n",
      " [[2.                   0.017857142857142856]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.1875              ]]\n",
      "\n",
      " [[0.                   0.8214285714285714  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.9196428571428571  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.5178571428571429  ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.7857142857142857  ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.39285714285714285 ]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.7678571428571429  ]]\n",
      "\n",
      " [[0.                   0.9107142857142857  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.044642857142857144]]\n",
      "\n",
      " [[2.                   0.044642857142857144]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.14285714285714285 ]]\n",
      "\n",
      " [[0.                   0.9821428571428571  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.9285714285714286  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.8035714285714286  ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.10714285714285714 ]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.44642857142857145 ]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.8392857142857143  ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.625               ]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.1875              ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.9017857142857143  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.7946428571428571  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.5803571428571429  ]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.26785714285714285 ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.044642857142857144]]\n",
      "\n",
      " [[2.                   0.044642857142857144]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.11607142857142858 ]]\n",
      "\n",
      " [[0.                   0.7589285714285714  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.5714285714285714  ]]\n",
      "\n",
      " [[0.                   0.6607142857142857  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.13392857142857142 ]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.5                 ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.9196428571428571  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.1875              ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.6160714285714286  ]]\n",
      "\n",
      " [[0.                   0.9196428571428571  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.1875              ]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.08035714285714286 ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.7946428571428571  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.9017857142857143  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"list pValue0\\n\",len(list_pValue0))\\nprint(\"list pValue1\\n\",list_pValue1)\\nprint(\"list pValue2\\n\",list_pValue2)\\ntot_list_pValues.append(list_pValue0)\\ntot_list_pValues.append(list_pValue1)\\ntot_list_pValues.append(list_pValue2)\\nprint(\"total list pValues\\n\",tot_list_pValues)\\n\\n\\n\\n#calulate pValue for assumed 1\\nfor i in range(len(list_score_test_sample_0_array)):\\n    pValueP = pValueZero(list_score_test_sample_0_array[i], zeroF[i])\\n    print(f\"p_vlaure for assumed 0 and all iteration \\n {pValueP} {i}\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define training data\n",
    "#train_samples = np.array([[0, 3], [2, 2], [3, 3], [-1, 1], [-1, -1], [0, 1]])\n",
    "#train_labels = np.array([1, 1, 1, -1, -1, -1])  # 1 = positive, -1 = negative\n",
    "list_score_test_sample_0 = []\n",
    "list_score_test_sample_1 = []\n",
    "list_score_test_sample_2 = []\n",
    "#count = 0\n",
    "seeZero = []\n",
    "zeroF = []\n",
    "seeOne = []\n",
    "oneF = []\n",
    "seeTwo = []\n",
    "twoF = []\n",
    "list_pValue0 = []\n",
    "list_pValue1 = []\n",
    "list_pValue2 = []\n",
    "tot_list_pValues = []\n",
    "tot_list_pValues2 = []\n",
    "for i in X_test: \n",
    "    train_samples = X_train\n",
    "    train_labels = y_train\n",
    "    #print(\"y_train \",len(y_train))\n",
    "    # Define test sample\n",
    "    #test_sample = np.array([0, 0])\n",
    "\n",
    "    test_sample = i\n",
    "\n",
    "    # Compute conformity scores assuming label 0    \n",
    "    #print(\"\\nConformity Scores assuming the test sample is 0:\")\n",
    "    conformity_scores_zero = compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label=0)\n",
    " \n",
    "    count = 0\n",
    "    for sample, label, score in conformity_scores_zero:\n",
    "        if(count <= 105):\n",
    "            seeZero.append(score)\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "        #print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score:.3f}\")\n",
    "    zeroF.append(seeZero.copy())\n",
    "    seeZero.clear()\n",
    "    #retrive all measure score for all test sample with assumed label 0\n",
    "    score_test_sample_0 = conformity_scores_zero[-1][2]\n",
    "    list_score_test_sample_0.append((0,score_test_sample_0))\n",
    "    list_score_test_sample_0_array = np.array(list_score_test_sample_0)\n",
    "\n",
    "\n",
    "    # Compute conformity scores assuming label 1\n",
    "    #print(\"\\nConformity Scores assuming the test sample is 1:\")\n",
    "    conformity_scores_one = compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label=1)\n",
    "\n",
    "    count = 0\n",
    "    for sample, label, score in conformity_scores_one:\n",
    "        if(count <= 105):\n",
    "            seeOne.append(score)\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "        #print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score:.3f}\")\n",
    "    oneF.append(seeOne.copy())\n",
    "    seeOne.clear()\n",
    "    #print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score:.3f}\")\n",
    "    #retrive all measure score for all test sample with assumed label 1\n",
    "    score_test_sample_1 = conformity_scores_one[-1][2]\n",
    "    list_score_test_sample_1.append((1,score_test_sample_1))\n",
    "    list_score_test_sample_1_array = np.array(list_score_test_sample_1)\n",
    "\n",
    "    # Compute conformity scores assuming label 2\n",
    "    #print(\"\\nConformity Scores assuming the test sample is 2:\")\n",
    "    conformity_scores_two = compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label=2)\n",
    "    count = 0\n",
    "    for sample, label, score in conformity_scores_two:\n",
    "        if(count <= 105):\n",
    "            seeTwo.append(score)\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "        #print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score:.3f}\")\n",
    "    twoF.append(seeTwo.copy())\n",
    "    seeTwo.clear()\n",
    "    #print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score:.3f}\")\n",
    "    \n",
    "     #retrive all measure score for all test sample with assumed label 2\n",
    "    score_test_sample_2 = conformity_scores_two[-1][2]\n",
    "    list_score_test_sample_2.append((2,score_test_sample_2))\n",
    "    list_score_test_sample_2_array = np.array(list_score_test_sample_2)\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=40)  # Suppress scientific notation, 5 decimal places\n",
    "#print(f\"list assumed label 0\\n{list_score_test_sample_0_array}\")\n",
    "#print(f\"list assumed label 1\\n{list_score_test_sample_1_array}\")\n",
    "#print(f\"list assumed label 2\\n{list_score_test_sample_2_array}\")\n",
    "\n",
    "#calulate pValue for assumed 0\n",
    "for i in range(len(list_score_test_sample_0_array)):\n",
    "    test_score_0 = list_score_test_sample_0_array[i][1]  # Extract the score from (label, score)\n",
    "    #print(\"ciao \",test_score_0)\n",
    "    conformity_score0 = zeroF[i]  # Get the corresponding list of scores from zeroF\n",
    "    \n",
    "    pValue0 = pValue(test_score_0, conformity_score0,key=0)\n",
    "    list_pValue0.append(pValue0)\n",
    "    #print(f\"p-value for assumed 0 at iteration {pValue0}\")\n",
    "\n",
    "#calulate pValue for assumed 1\n",
    "for i in range(len(list_score_test_sample_0_array)):\n",
    "    test_score_One = list_score_test_sample_1_array[i][1]  # Extract the score from (label, score)\n",
    "    #print(\"ciao \",test_score_One)\n",
    "    conformity_score1 = oneF[i]  # Get the corresponding list of scores from zeroF\n",
    "    \n",
    "    pValue1 = pValue(test_score_One, conformity_score1,key=1)\n",
    "    list_pValue1.append(pValue1)\n",
    "\n",
    "    #print(f\"p-value for assumed 1 at iteration {pValue1}\")\n",
    "\n",
    "#calulate pValue for assumed 2\n",
    "for i in range(len(list_score_test_sample_2_array)):\n",
    "    test_score_Two = list_score_test_sample_2_array[i][1]  # Extract the score from (label, score)\n",
    "    #print(\"ciao \",test_score_Two)\n",
    "    conformity_score2 = twoF[i]  # Get the corresponding list of scores from zeroF\n",
    "    \n",
    "    pValue2 = pValue(test_score_Two, conformity_score2,key=2)\n",
    "    list_pValue2.append(pValue2)\n",
    "\n",
    "    #print(f\"p-value for assumed 2 at iteration {pValue2}\")\n",
    "\n",
    "for i in range(len(list_pValue0)):\n",
    "        tot_list_pValues2.append(list_pValue0[i])\n",
    "        tot_list_pValues2.append(list_pValue1[i])\n",
    "        tot_list_pValues2.append(list_pValue2[i])\n",
    "\n",
    "convert_array = np.array(tot_list_pValues2)\n",
    "\n",
    "print(\"convert_array\\n\", convert_array)\n",
    "'''\n",
    "print(\"list pValue0\\n\",len(list_pValue0))\n",
    "print(\"list pValue1\\n\",list_pValue1)\n",
    "print(\"list pValue2\\n\",list_pValue2)\n",
    "tot_list_pValues.append(list_pValue0)\n",
    "tot_list_pValues.append(list_pValue1)\n",
    "tot_list_pValues.append(list_pValue2)\n",
    "print(\"total list pValues\\n\",tot_list_pValues)\n",
    "\n",
    "\n",
    "\n",
    "#calulate pValue for assumed 1\n",
    "for i in range(len(list_score_test_sample_0_array)):\n",
    "    pValueP = pValueZero(list_score_test_sample_0_array[i], zeroF[i])\n",
    "    print(f\"p_vlaure for assumed 0 and all iteration \\n {pValueP} {i}\")\n",
    "'''\n",
    "#print(f\"sees \\n {zeroF[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average false p_value  0.012335526315789479\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for i in range(len(list_pValue0)):\n",
    "        #print(list_pValue0[i][0][0])\n",
    "        #print(list_pValue0[i][0][1])        \n",
    "'''\n",
    "sum = 0\n",
    "count = 0\n",
    "for i in  range(len(y_test)):\n",
    "        #print(label,list_pValue0[i][0][0])\n",
    "        if(y_test[i] == 0):\n",
    "                #print(\"first \",list_pValue0[i][0][0],y_test[i])\n",
    "                sum += list_pValue1[i][0][1] + list_pValue2[i][0][1] \n",
    "                count += 2\n",
    "                #print(\"counter first\", count)\n",
    "        elif(y_test[i]== 1):\n",
    "                #print(\"second \",list_pValue1[i][0][0],y_test[i])\n",
    "                sum += list_pValue0[i][0][1] + list_pValue2[i][0][1]\n",
    "                count += 2\n",
    "                #print(\"counter secornd\", count)\n",
    "        elif(y_test[i] == 2):\n",
    "                #print(\"third \",list_pValue2[i][0][0],y_test[i])\n",
    "                sum += list_pValue0[i][0][1] + list_pValue1[i][0][1]\n",
    "                count += 2\n",
    "                #print(\"counter third\", count)\n",
    "\n",
    "#print(sum)         \n",
    "#print(\"count \",count)\n",
    "tot = sum /count\n",
    "print(\"average false p_value \", tot)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
