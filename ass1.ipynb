{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "#question1\n",
    "from numpy import mean\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=2404)\n",
    "\n",
    "print(len(y_test))\n",
    "#print(iris)\n",
    "#print(iris.data.shape) #\n",
    "#print(X_train[0])\n",
    "#print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calulate euclidian distance\n",
    "from math import sqrt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def distance(a,b):\n",
    "    return np.sqrt(np.sum((a-b)**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurancy nn  0.9473684210526315\n",
      "error rate  0.052631578947368474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 0, 1, 1, 2, 2, 0, 2, 2, 0, 0, 1, 2, 2, 1, 2, 1, 0, 0, 2,\n",
       "       1, 2, 2, 0, 2, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 0])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#question 3\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "#knn 1\n",
    "def nn(X_train,X_test,y_train):\n",
    "\n",
    "    #print(len(y_test))\n",
    "    pred_labels = []\n",
    "    #distances = []\n",
    "    for test_sample in X_test:\n",
    "        distances =  [distance(test_sample, train_sample) for train_sample in X_train]\n",
    "        #stroe all distance for each test\n",
    "        #distances.append(dist)\n",
    "        #find distance of minimum distance\n",
    "        index_min_dist = np.argmin(distances)\n",
    "        #print(index_min_dist)\n",
    "        pred_labels.append(y_train[index_min_dist])\n",
    "        pred_labels_convert_array = np.array(pred_labels)\n",
    "    #print(\"y_test \",y_test)\n",
    "    #print(\"pred_y \",pred_labels_convert_array )\n",
    "    \n",
    "    print(\"accurancy nn \",np.mean(y_test ==pred_labels_convert_array))\n",
    "    print(\"error rate \", 1 - np.mean(y_test ==pred_labels_convert_array))\n",
    "        #print(distances)\n",
    "        \n",
    "    return pred_labels_convert_array\n",
    "nn(X_train,X_test,y_train)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy k=3  0.9473684210526315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 0, 1, 1, 2, 2, 0, 2, 2, 0, 0, 1, 2, 2, 1, 2, 1, 0, 0, 2,\n",
       "       1, 2, 2, 0, 2, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knn 3\n",
    "def knn(X_train, X_test, y_train, k):\n",
    "    pred_labels = []\n",
    "    for test_sample in X_test:\n",
    "        distances = [distance(test_sample, train_sample) for train_sample in X_train]\n",
    "        index_distances_k = np.argsort(distances)[:k] #get the first 3 nearest distances in term of index\n",
    "        #print(\"list of 3 distances \", index_distances_k)\n",
    "        nearest_labels = [y_train[i]  for i in index_distances_k] #get the three labels\n",
    "        #print(\"list of 3 lables \",nearest_labels)\n",
    "        majority_vote = Counter(nearest_labels).most_common(1)[0][0] #get tuple(key, number times appear) and store anly the key.\n",
    "        #print(majority_vote)\n",
    "        pred_labels.append(majority_vote)\n",
    "        pred_labels_convert_array3 = np.array(pred_labels)\n",
    "        \n",
    "\n",
    "\n",
    "    #print(\"pred_labels_convert_array \",pred_labels_convert_array3)\n",
    "    print(\"accuracy k=3 \",np.mean(y_test ==pred_labels_convert_array3))\n",
    "    return pred_labels_convert_array3\n",
    "#nn(X_train,X_test,y_train)\n",
    "knn(X_train,X_test,y_train,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy k=3  0.9473684210526315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 0, 1, 1, 2, 2, 0, 2, 2, 0, 0, 1, 2, 2, 1, 2, 1, 0, 0, 2,\n",
       "       1, 2, 2, 0, 2, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn(X_train,X_test,y_train,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy k=3  0.9473684210526315\n",
      "accurancy nn  0.9473684210526315\n",
      "error rate  0.052631578947368474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(knn(X_train,X_test,y_train,3) == nn(X_train,X_test,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport numpy as np\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom collections import Counter\\n\\n# Load the Iris dataset\\ndata = load_iris()\\nX = data.data\\ny = data.target\\n\\n# Split the dataset into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2404)\\n\\nprint(X_test[0])\\nprint(X_train[0])\\n\\ndef euclidean_distance(x1, x2):\\n    return np.sqrt(np.sum((x1 - x2) ** 2))\\n\\n# Nearest Neighbour Algorithm\\ndef nearest_neighbour(X_train, y_train, X_test):\\n    predictions = []\\n    for test_sample in X_test:\\n        #print(test_sample)\\n        distances = [euclidean_distance(test_sample, train_sample) for train_sample in X_train]\\n        #print(\"dit\",distances)\\n        nearest_index = np.argmin(distances)\\n        predictions.append(y_train[nearest_index])\\n    #print(len(predictions))\\n    return predictions\\n\\n# K-Nearest Neighbours Algorithm\\ndef k_nearest_neighbours(X_train, y_train, X_test, k=3):\\n    predictions = []\\n    for test_sample in X_test:\\n        distances = [euclidean_distance(test_sample, train_sample) for train_sample in X_train]\\n        nearest_indices = np.argsort(distances)[:k]\\n        nearest_labels = [y_train[i] for i in nearest_indices]\\n        majority_vote = Counter(nearest_labels).most_common(1)[0][0]\\n        predictions.append(majority_vote)\\n    return predictions\\n\\n#print(euclidean_distance(X_test[0],X_train[0]))\\n\\n# Evaluate the Nearest Neighbour Algorithm\\nnn_predictions = nearest_neighbour(X_train, y_train, X_test)\\nnn_accuracy = np.mean(nn_predictions == y_test)\\nprint(f\"Nearest Neighbour Accuracy: {nn_accuracy:.2f}\")\\n\\n# Evaluate the K-Nearest Neighbours Algorithm\\nk = 3\\nknn_predictions = k_nearest_neighbours(X_train, y_train, X_test, k=k)\\nknn_accuracy = np.mean(knn_predictions == y_test)\\nprint(f\"K-Nearest Neighbours (k={k}) Accuracy: {knn_accuracy:.2f}\")\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2404)\n",
    "\n",
    "print(X_test[0])\n",
    "print(X_train[0])\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "# Nearest Neighbour Algorithm\n",
    "def nearest_neighbour(X_train, y_train, X_test):\n",
    "    predictions = []\n",
    "    for test_sample in X_test:\n",
    "        #print(test_sample)\n",
    "        distances = [euclidean_distance(test_sample, train_sample) for train_sample in X_train]\n",
    "        #print(\"dit\",distances)\n",
    "        nearest_index = np.argmin(distances)\n",
    "        predictions.append(y_train[nearest_index])\n",
    "    #print(len(predictions))\n",
    "    return predictions\n",
    "\n",
    "# K-Nearest Neighbours Algorithm\n",
    "def k_nearest_neighbours(X_train, y_train, X_test, k=3):\n",
    "    predictions = []\n",
    "    for test_sample in X_test:\n",
    "        distances = [euclidean_distance(test_sample, train_sample) for train_sample in X_train]\n",
    "        nearest_indices = np.argsort(distances)[:k]\n",
    "        nearest_labels = [y_train[i] for i in nearest_indices]\n",
    "        majority_vote = Counter(nearest_labels).most_common(1)[0][0]\n",
    "        predictions.append(majority_vote)\n",
    "    return predictions\n",
    "\n",
    "#print(euclidean_distance(X_test[0],X_train[0]))\n",
    "\n",
    "# Evaluate the Nearest Neighbour Algorithm\n",
    "nn_predictions = nearest_neighbour(X_train, y_train, X_test)\n",
    "nn_accuracy = np.mean(nn_predictions == y_test)\n",
    "print(f\"Nearest Neighbour Accuracy: {nn_accuracy:.2f}\")\n",
    "\n",
    "# Evaluate the K-Nearest Neighbours Algorithm\n",
    "k = 3\n",
    "knn_predictions = k_nearest_neighbours(X_train, y_train, X_test, k=k)\n",
    "knn_accuracy = np.mean(knn_predictions == y_test)\n",
    "print(f\"K-Nearest Neighbours (k={k}) Accuracy: {knn_accuracy:.2f}\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load the Iris dataset\\ndata = load_iris()\\nX = data.data\\ny = data.target\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y , random_state=2404)\\n\\ndef euclidean_distance(x1, x2):\\n    return np.sqrt(np.sum((x1 - x2) ** 2))\\n\\ndef conformity_measure(X_train, y_train, X_test, y_test):\\n    conformity_scores = []\\n    for i, test_sample in enumerate(X_test):\\n        same_class_dist = float(\\'inf\\')\\n        diff_class_dist = float(\\'inf\\')\\n        \\n        for j, train_sample in enumerate(X_train):\\n            dist = distance(test_sample, train_sample)\\n            #dist = euclidean_distance(test_sample, train_sample)\\n            \\n            if y_train[j] == y_test[i]:  # Nearest sample of the same class\\n                if dist < same_class_dist:\\n                    same_class_dist = dist\\n            else:  # Nearest sample of a different class\\n                if dist < diff_class_dist:\\n                    diff_class_dist = dist\\n        \\n        # Calculate the conformity measure\\n        if same_class_dist > 0:  # Prevent division by zero\\n            conformity = diff_class_dist / same_class_dist\\n        else:\\n            conformity = float(\\'inf\\')\\n        \\n        conformity_scores.append(conformity)\\n        print(f\"Test sample {i}: Same Class Dist = {same_class_dist:.3f}, \"\\n              f\"Different Class Dist = {diff_class_dist:.3f}, \"\\n              f\"Conformity = {conformity:.3f}\")\\n    \\n    return conformity_scores\\n\\n# Compute conformity measures\\n#conformity_scores = conformity_measure(X_train, y_train, X_test, y_test)\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , random_state=2404)\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "def conformity_measure(X_train, y_train, X_test, y_test):\n",
    "    conformity_scores = []\n",
    "    for i, test_sample in enumerate(X_test):\n",
    "        same_class_dist = float('inf')\n",
    "        diff_class_dist = float('inf')\n",
    "        \n",
    "        for j, train_sample in enumerate(X_train):\n",
    "            dist = distance(test_sample, train_sample)\n",
    "            #dist = euclidean_distance(test_sample, train_sample)\n",
    "            \n",
    "            if y_train[j] == y_test[i]:  # Nearest sample of the same class\n",
    "                if dist < same_class_dist:\n",
    "                    same_class_dist = dist\n",
    "            else:  # Nearest sample of a different class\n",
    "                if dist < diff_class_dist:\n",
    "                    diff_class_dist = dist\n",
    "        \n",
    "        # Calculate the conformity measure\n",
    "        if same_class_dist > 0:  # Prevent division by zero\n",
    "            conformity = diff_class_dist / same_class_dist\n",
    "        else:\n",
    "            conformity = float('inf')\n",
    "        \n",
    "        conformity_scores.append(conformity)\n",
    "        print(f\"Test sample {i}: Same Class Dist = {same_class_dist:.3f}, \"\n",
    "              f\"Different Class Dist = {diff_class_dist:.3f}, \"\n",
    "              f\"Conformity = {conformity:.3f}\")\n",
    "    \n",
    "    return conformity_scores\n",
    "\n",
    "# Compute conformity measures\n",
    "#conformity_scores = conformity_measure(X_train, y_train, X_test, y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport numpy as np\\n\\ndef euclidean_distance(x1, x2):\\n    return np.sqrt(np.sum((x1 - x2) ** 2))\\n\\ndef conformity_measure(X_train, y_train, X_test, assumed_labels, epsilon=1e-8):\\n    conformity_scores = []\\n\\n    # Repeat assumed_label for all test samples\\n    assumed_labels = np.full(len(X_test), assumed_labels)\\n    \\n    # Extend the training samples and labels\\n    extended_samples = np.vstack([X_train, X_test])\\n    extended_labels = np.append(y_train, assumed_labels)\\n\\n    # Compute conformity scores for the test samples\\n    for i, sample in enumerate(X_test):  # Only iterate over test samples\\n        same_class_dist = float(\\'inf\\')\\n        diff_class_dist = float(\\'inf\\')\\n\\n        for j, train_sample in enumerate(extended_samples):\\n            dist = euclidean_distance(sample, train_sample)\\n\\n            if extended_labels[j] == assumed_labels[i]:  # Same class\\n                same_class_dist = min(same_class_dist, dist)\\n            else:  # Different class\\n                diff_class_dist = min(diff_class_dist, dist)\\n\\n        # Handle division by zero\\n        if same_class_dist == 0.0:\\n            same_class_dist += epsilon\\n        \\n        conformity = diff_class_dist / same_class_dist\\n        conformity_scores.append((sample, assumed_labels[i], conformity))\\n\\n    return conformity_scores\\n\\n\\n# Example usage with Iris dataset\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load the Iris dataset\\ndata = load_iris()\\nX = data.data\\ny = data.target\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2404)\\n\\n# Evaluate for each assumed label (0, 1, 2)\\nfor label in [0, 1, 2]:\\n    print(f\"\\nConformity Scores assuming label {label}:\")\\n    conformity_scores = conformity_measure(X_train, y_train, X_test, assumed_labels=label)\\n    #print(len(conformity_scores))\\n    for sample, assumed_label, score in conformity_scores:\\n        print(f\"Test Sample: {sample}, Assumed Label: {assumed_label}, Conformity Score: {score:.3f}\")\\nprint(len(conformity_scores))\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "def conformity_measure(X_train, y_train, X_test, assumed_labels, epsilon=1e-8):\n",
    "    conformity_scores = []\n",
    "\n",
    "    # Repeat assumed_label for all test samples\n",
    "    assumed_labels = np.full(len(X_test), assumed_labels)\n",
    "    \n",
    "    # Extend the training samples and labels\n",
    "    extended_samples = np.vstack([X_train, X_test])\n",
    "    extended_labels = np.append(y_train, assumed_labels)\n",
    "\n",
    "    # Compute conformity scores for the test samples\n",
    "    for i, sample in enumerate(X_test):  # Only iterate over test samples\n",
    "        same_class_dist = float('inf')\n",
    "        diff_class_dist = float('inf')\n",
    "\n",
    "        for j, train_sample in enumerate(extended_samples):\n",
    "            dist = euclidean_distance(sample, train_sample)\n",
    "\n",
    "            if extended_labels[j] == assumed_labels[i]:  # Same class\n",
    "                same_class_dist = min(same_class_dist, dist)\n",
    "            else:  # Different class\n",
    "                diff_class_dist = min(diff_class_dist, dist)\n",
    "\n",
    "        # Handle division by zero\n",
    "        if same_class_dist == 0.0:\n",
    "            same_class_dist += epsilon\n",
    "        \n",
    "        conformity = diff_class_dist / same_class_dist\n",
    "        conformity_scores.append((sample, assumed_labels[i], conformity))\n",
    "\n",
    "    return conformity_scores\n",
    "\n",
    "\n",
    "# Example usage with Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2404)\n",
    "\n",
    "# Evaluate for each assumed label (0, 1, 2)\n",
    "for label in [0, 1, 2]:\n",
    "    print(f\"\\nConformity Scores assuming label {label}:\")\n",
    "    conformity_scores = conformity_measure(X_train, y_train, X_test, assumed_labels=label)\n",
    "    #print(len(conformity_scores))\n",
    "    for sample, assumed_label, score in conformity_scores:\n",
    "        print(f\"Test Sample: {sample}, Assumed Label: {assumed_label}, Conformity Score: {score:.3f}\")\n",
    "print(len(conformity_scores))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef conformity_measure(X_train, y_train, X_test, possible_labels=[0, 1, 2]):\\n    conformity_scores = []\\n\\n    # Loop through each test sample\\n    print(\"test sample \",len(X_test))\\n    print(\"train sample \",len(X))\\n    for i, test_sample in enumerate(X_test):\\n        scores_for_sample = {}  # Dictionary to store scores for assumed labels\\n        \\n        # Assume each possible label and calculate conformity measure\\n        for assumed_label in possible_labels:\\n            same_class_dist = float(\\'inf\\')\\n            diff_class_dist = float(\\'inf\\')\\n            \\n            for j, train_sample in enumerate(X_train):\\n                dist = euclidean_distance(test_sample, train_sample)\\n                \\n                if y_train[j] == assumed_label:  # Same class\\n                    same_class_dist = min(same_class_dist, dist)\\n                else:  # Different class\\n                    diff_class_dist = min(diff_class_dist, dist)\\n            \\n            # Calculate the conformity measure\\n            if same_class_dist > 0:  # Prevent division by zero\\n                conformity = diff_class_dist / same_class_dist\\n            else:\\n                conformity = float(\\'inf\\')\\n            \\n            # Store the conformity score for this assumed label\\n            scores_for_sample[assumed_label] = conformity\\n        \\n        # Append the scores for this test sample\\n        conformity_scores.append(scores_for_sample)\\n        \\n        # Print the results for debugging\\n        print(f\"Test Sample {i}: {scores_for_sample}\")\\n    \\n    return conformity_scores\\n\\n\\n\\ndef calculate_p_value(X_train, y_train, X_test, y_test):\\n    conformity_scores_train = []\\n    \\n    # Step 1: Compute conformity scores for training samples\\n    for i, train_sample in enumerate(X_train):\\n        score = conformity_measure(X_train, y_train, train_sample, y_train[i], exclude_index=i)\\n        conformity_scores_train.append(score)\\n    \\n    p_values = []\\n\\n    # Step 2: Compute p-values for test samples\\n    for i, test_sample in enumerate(X_test):\\n        test_conformity = conformity_measure(X_train, y_train, test_sample, y_test[i])\\n        \\n        # Combine training scores and the test conformity score\\n        combined_scores = conformity_scores_train + [test_conformity]\\n        \\n        # Count scores greater than or equal to the test conformity score\\n        count = sum(1 for score in combined_scores if score <= test_conformity)\\n        p_value = count / len(combined_scores)\\n        \\n        #p_values.append(p_value)\\n         \\n        #print(f\"Combined Scores: {combined_scores}\")\\n        #print(f\"Test Sample {i}: p-value = {p_value:.3f}\")\\n        \\n        p_values.append((y_test[i],p_value))\\n        #print(\"Number of test samples:\", len(X_test))\\n        #print(len(p_values))\\n    \\n    return p_values\\nonformity_scores = conformity_measure(X_train, y_train, X_test, y_test)\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "def conformity_measure(X_train, y_train, X_test, possible_labels=[0, 1, 2]):\n",
    "    conformity_scores = []\n",
    "\n",
    "    # Loop through each test sample\n",
    "    print(\"test sample \",len(X_test))\n",
    "    print(\"train sample \",len(X))\n",
    "    for i, test_sample in enumerate(X_test):\n",
    "        scores_for_sample = {}  # Dictionary to store scores for assumed labels\n",
    "        \n",
    "        # Assume each possible label and calculate conformity measure\n",
    "        for assumed_label in possible_labels:\n",
    "            same_class_dist = float('inf')\n",
    "            diff_class_dist = float('inf')\n",
    "            \n",
    "            for j, train_sample in enumerate(X_train):\n",
    "                dist = euclidean_distance(test_sample, train_sample)\n",
    "                \n",
    "                if y_train[j] == assumed_label:  # Same class\n",
    "                    same_class_dist = min(same_class_dist, dist)\n",
    "                else:  # Different class\n",
    "                    diff_class_dist = min(diff_class_dist, dist)\n",
    "            \n",
    "            # Calculate the conformity measure\n",
    "            if same_class_dist > 0:  # Prevent division by zero\n",
    "                conformity = diff_class_dist / same_class_dist\n",
    "            else:\n",
    "                conformity = float('inf')\n",
    "            \n",
    "            # Store the conformity score for this assumed label\n",
    "            scores_for_sample[assumed_label] = conformity\n",
    "        \n",
    "        # Append the scores for this test sample\n",
    "        conformity_scores.append(scores_for_sample)\n",
    "        \n",
    "        # Print the results for debugging\n",
    "        print(f\"Test Sample {i}: {scores_for_sample}\")\n",
    "    \n",
    "    return conformity_scores\n",
    "\n",
    "\n",
    "\n",
    "def calculate_p_value(X_train, y_train, X_test, y_test):\n",
    "    conformity_scores_train = []\n",
    "    \n",
    "    # Step 1: Compute conformity scores for training samples\n",
    "    for i, train_sample in enumerate(X_train):\n",
    "        score = conformity_measure(X_train, y_train, train_sample, y_train[i], exclude_index=i)\n",
    "        conformity_scores_train.append(score)\n",
    "    \n",
    "    p_values = []\n",
    "\n",
    "    # Step 2: Compute p-values for test samples\n",
    "    for i, test_sample in enumerate(X_test):\n",
    "        test_conformity = conformity_measure(X_train, y_train, test_sample, y_test[i])\n",
    "        \n",
    "        # Combine training scores and the test conformity score\n",
    "        combined_scores = conformity_scores_train + [test_conformity]\n",
    "        \n",
    "        # Count scores greater than or equal to the test conformity score\n",
    "        count = sum(1 for score in combined_scores if score <= test_conformity)\n",
    "        p_value = count / len(combined_scores)\n",
    "        \n",
    "        #p_values.append(p_value)\n",
    "         \n",
    "        #print(f\"Combined Scores: {combined_scores}\")\n",
    "        #print(f\"Test Sample {i}: p-value = {p_value:.3f}\")\n",
    "        \n",
    "        p_values.append((y_test[i],p_value))\n",
    "        #print(\"Number of test samples:\", len(X_test))\n",
    "        #print(len(p_values))\n",
    "    \n",
    "    return p_values\n",
    "onformity_scores = conformity_measure(X_train, y_train, X_test, y_test)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef conformity_measure_with_train(X_train, y_train, X_test, possible_labels=[0, 1, 2]):\\n    conformity_scores = []\\n    \\n    # Step 1: Calculate conformity scores for TRAIN samples (use true labels)\\n    print(\"Calculating conformity scores for TRAIN samples...\")\\n    for i, train_sample in enumerate(X_train):\\n        same_class_dist = float(\\'inf\\')\\n        diff_class_dist = float(\\'inf\\')\\n        \\n        for j, other_sample in enumerate(X_train):\\n            if i == j:  # Skip comparison with itself\\n                continue\\n            \\n            dist = euclidean_distance(train_sample, other_sample)\\n            if y_train[j] == y_train[i]:  # Same class\\n                same_class_dist = min(same_class_dist, dist)\\n            else:  # Different class\\n                diff_class_dist = min(diff_class_dist, dist)\\n        \\n        # Prevent division by zero\\n        if same_class_dist > 0:\\n            conformity = diff_class_dist / same_class_dist\\n        else:\\n            conformity = float(\\'inf\\')\\n        \\n        conformity_scores.append((\"Train\", i, y_train[i], conformity))\\n        print(f\"Train Sample {i}: Label: {y_train[i]}, Conformity Score: {conformity:.3f}\")\\n    \\n    # Step 2: Calculate conformity scores for TEST samples with assumed labels\\n    print(\"\\nCalculating conformity scores for TEST samples...\")\\n    for i, test_sample in enumerate(X_test):\\n        scores_for_sample = {}  # Dictionary to store scores for assumed labels\\n        \\n        for assumed_label in possible_labels:\\n            same_class_dist = float(\\'inf\\')\\n            diff_class_dist = float(\\'inf\\')\\n            \\n            for j, train_sample in enumerate(X_train):\\n                dist = euclidean_distance(test_sample, train_sample)\\n                if y_train[j] == assumed_label:  # Same class\\n                    same_class_dist = min(same_class_dist, dist)\\n                else:  # Different class\\n                    diff_class_dist = min(diff_class_dist, dist)\\n            \\n            # Prevent division by zero\\n            if same_class_dist > 0:\\n                conformity = diff_class_dist / same_class_dist\\n            else:\\n                conformity = float(\\'inf\\')\\n            \\n            scores_for_sample[assumed_label] = conformity\\n        \\n        # Append conformity scores for this test sample\\n        conformity_scores.append((\"Test\", i, scores_for_sample))\\n        print(f\"Test Sample {i}: {scores_for_sample}\")\\n    \\n    return conformity_scores\\n\\nonformity_scores = conformity_measure_with_train(X_train, y_train, X_test, y_test)\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def conformity_measure_with_train(X_train, y_train, X_test, possible_labels=[0, 1, 2]):\n",
    "    conformity_scores = []\n",
    "    \n",
    "    # Step 1: Calculate conformity scores for TRAIN samples (use true labels)\n",
    "    print(\"Calculating conformity scores for TRAIN samples...\")\n",
    "    for i, train_sample in enumerate(X_train):\n",
    "        same_class_dist = float('inf')\n",
    "        diff_class_dist = float('inf')\n",
    "        \n",
    "        for j, other_sample in enumerate(X_train):\n",
    "            if i == j:  # Skip comparison with itself\n",
    "                continue\n",
    "            \n",
    "            dist = euclidean_distance(train_sample, other_sample)\n",
    "            if y_train[j] == y_train[i]:  # Same class\n",
    "                same_class_dist = min(same_class_dist, dist)\n",
    "            else:  # Different class\n",
    "                diff_class_dist = min(diff_class_dist, dist)\n",
    "        \n",
    "        # Prevent division by zero\n",
    "        if same_class_dist > 0:\n",
    "            conformity = diff_class_dist / same_class_dist\n",
    "        else:\n",
    "            conformity = float('inf')\n",
    "        \n",
    "        conformity_scores.append((\"Train\", i, y_train[i], conformity))\n",
    "        print(f\"Train Sample {i}: Label: {y_train[i]}, Conformity Score: {conformity:.3f}\")\n",
    "    \n",
    "    # Step 2: Calculate conformity scores for TEST samples with assumed labels\n",
    "    print(\"\\nCalculating conformity scores for TEST samples...\")\n",
    "    for i, test_sample in enumerate(X_test):\n",
    "        scores_for_sample = {}  # Dictionary to store scores for assumed labels\n",
    "        \n",
    "        for assumed_label in possible_labels:\n",
    "            same_class_dist = float('inf')\n",
    "            diff_class_dist = float('inf')\n",
    "            \n",
    "            for j, train_sample in enumerate(X_train):\n",
    "                dist = euclidean_distance(test_sample, train_sample)\n",
    "                if y_train[j] == assumed_label:  # Same class\n",
    "                    same_class_dist = min(same_class_dist, dist)\n",
    "                else:  # Different class\n",
    "                    diff_class_dist = min(diff_class_dist, dist)\n",
    "            \n",
    "            # Prevent division by zero\n",
    "            if same_class_dist > 0:\n",
    "                conformity = diff_class_dist / same_class_dist\n",
    "            else:\n",
    "                conformity = float('inf')\n",
    "            \n",
    "            scores_for_sample[assumed_label] = conformity\n",
    "        \n",
    "        # Append conformity scores for this test sample\n",
    "        conformity_scores.append((\"Test\", i, scores_for_sample))\n",
    "        print(f\"Test Sample {i}: {scores_for_sample}\")\n",
    "    \n",
    "    return conformity_scores\n",
    "\n",
    "onformity_scores = conformity_measure_with_train(X_train, y_train, X_test, y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load the Iris dataset\\ndata = load_iris()\\nX = data.data\\ny = data.target\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\\n\\ndef euclidean_distance(x1, x2):\\n    return np.sqrt(np.sum((x1 - x2) ** 2))\\n\\ndef conformity_measure(X_train, y_train, sample, label, exclude_index=None):\\n    same_class_dist = float(\\'inf\\')\\n    diff_class_dist = float(\\'inf\\')\\n\\n    for i, train_sample in enumerate(X_train):\\n        # Skip self-comparison if exclude_index is provided\\n        if exclude_index is not None and i == exclude_index:\\n            continue\\n\\n        dist = np.sqrt(np.sum((sample - train_sample) ** 2))  # Euclidean distance\\n\\n        if y_train[i] == label:  # Same class\\n            same_class_dist = min(same_class_dist, dist)\\n        else:  # Different class\\n            diff_class_dist = min(diff_class_dist, dist)\\n\\n    # Handle division by zero properly\\n    if same_class_dist > 0:\\n        return diff_class_dist / same_class_dist\\n    else:\\n        return float(\\'inf\\')\\n\\n\\ndef calculate_p_value(X_train, y_train, X_test, y_test):\\n    conformity_scores_train = []\\n    \\n    # Step 1: Compute conformity scores for training samples\\n    for i, train_sample in enumerate(X_train):\\n        score = conformity_measure(X_train, y_train, train_sample, y_train[i], exclude_index=i)\\n        conformity_scores_train.append(score)\\n    \\n    p_values = []\\n\\n    # Step 2: Compute p-values for test samples\\n    for i, test_sample in enumerate(X_test):\\n        test_conformity = conformity_measure(X_train, y_train, test_sample, y_test[i])\\n        \\n        # Combine training scores and the test conformity score\\n        combined_scores = conformity_scores_train + [test_conformity]\\n        \\n        # Count scores greater than or equal to the test conformity score\\n        count = sum(1 for score in combined_scores if score <= test_conformity)\\n        p_value = count / len(combined_scores)\\n        \\n        #p_values.append(p_value)\\n         \\n        #print(f\"Combined Scores: {combined_scores}\")\\n        #print(f\"Test Sample {i}: p-value = {p_value:.3f}\")\\n        \\n        p_values.append((y_test[i],p_value))\\n        #print(\"Number of test samples:\", len(X_test))\\n        #print(len(p_values))\\n    \\n    return p_values\\n\\n\\n# Calculate p-values for test samples\\np_values = calculate_p_value(X_train, y_train, X_test, y_test)\\nprint(p_values)\\nprint(p_values[1][0])\\nprint(p_values[0][0] == (y_test[0]))\\n\\n#average error p_value\\ndef average_error_p_value(p_values):\\n\\n    return p_values\\n\\nave = average_error_p_value(p_values)\\n\\nprint(\"average \",)\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "def conformity_measure(X_train, y_train, sample, label, exclude_index=None):\n",
    "    same_class_dist = float('inf')\n",
    "    diff_class_dist = float('inf')\n",
    "\n",
    "    for i, train_sample in enumerate(X_train):\n",
    "        # Skip self-comparison if exclude_index is provided\n",
    "        if exclude_index is not None and i == exclude_index:\n",
    "            continue\n",
    "\n",
    "        dist = np.sqrt(np.sum((sample - train_sample) ** 2))  # Euclidean distance\n",
    "\n",
    "        if y_train[i] == label:  # Same class\n",
    "            same_class_dist = min(same_class_dist, dist)\n",
    "        else:  # Different class\n",
    "            diff_class_dist = min(diff_class_dist, dist)\n",
    "\n",
    "    # Handle division by zero properly\n",
    "    if same_class_dist > 0:\n",
    "        return diff_class_dist / same_class_dist\n",
    "    else:\n",
    "        return float('inf')\n",
    "\n",
    "\n",
    "def calculate_p_value(X_train, y_train, X_test, y_test):\n",
    "    conformity_scores_train = []\n",
    "    \n",
    "    # Step 1: Compute conformity scores for training samples\n",
    "    for i, train_sample in enumerate(X_train):\n",
    "        score = conformity_measure(X_train, y_train, train_sample, y_train[i], exclude_index=i)\n",
    "        conformity_scores_train.append(score)\n",
    "    \n",
    "    p_values = []\n",
    "\n",
    "    # Step 2: Compute p-values for test samples\n",
    "    for i, test_sample in enumerate(X_test):\n",
    "        test_conformity = conformity_measure(X_train, y_train, test_sample, y_test[i])\n",
    "        \n",
    "        # Combine training scores and the test conformity score\n",
    "        combined_scores = conformity_scores_train + [test_conformity]\n",
    "        \n",
    "        # Count scores greater than or equal to the test conformity score\n",
    "        count = sum(1 for score in combined_scores if score <= test_conformity)\n",
    "        p_value = count / len(combined_scores)\n",
    "        \n",
    "        #p_values.append(p_value)\n",
    "         \n",
    "        #print(f\"Combined Scores: {combined_scores}\")\n",
    "        #print(f\"Test Sample {i}: p-value = {p_value:.3f}\")\n",
    "        \n",
    "        p_values.append((y_test[i],p_value))\n",
    "        #print(\"Number of test samples:\", len(X_test))\n",
    "        #print(len(p_values))\n",
    "    \n",
    "    return p_values\n",
    "\n",
    "\n",
    "# Calculate p-values for test samples\n",
    "p_values = calculate_p_value(X_train, y_train, X_test, y_test)\n",
    "print(p_values)\n",
    "print(p_values[1][0])\n",
    "print(p_values[0][0] == (y_test[0]))\n",
    "\n",
    "#average error p_value\n",
    "def average_error_p_value(p_values):\n",
    "\n",
    "    return p_values\n",
    "\n",
    "ave = average_error_p_value(p_values)\n",
    "\n",
    "print(\"average \",)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label, epsilon=1e-8):\n",
    "    results = []\n",
    "\n",
    "    # Add the test sample with assumed label to the training set temporarily\n",
    "    extended_samples = np.vstack([train_samples, test_sample])\n",
    "    extended_labels = np.append(train_labels, assumed_label)\n",
    "\n",
    "    # Compute conformity scores for each sample in the extended training set\n",
    "    for i, sample in enumerate(extended_samples):\n",
    "        same_class_dist = float('inf')\n",
    "        diff_class_dist = float('inf')\n",
    "\n",
    "        for j, other_sample in enumerate(extended_samples):\n",
    "            if i == j:  # Skip comparison with itself\n",
    "                continue\n",
    "            \n",
    "            dist = distance(sample, other_sample)\n",
    "\n",
    "            if extended_labels[j] == extended_labels[i]:  # Same class\n",
    "                same_class_dist = min(same_class_dist, dist)\n",
    "            else:  # Different class\n",
    "                diff_class_dist = min(diff_class_dist, dist)\n",
    "\n",
    "        # Handle division by zero\n",
    "        if same_class_dist == 0.0:\n",
    "            same_class_dist += epsilon\n",
    "        \n",
    "        conformity = diff_class_dist / same_class_dist\n",
    "        results.append((sample, extended_labels[i], conformity))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calulate pValue for assumed 0\n",
    "def pValue(test_score_neg, confScore0,key):\n",
    "    list_pValue_with_lable0  = []\n",
    "\n",
    "    rank = 0\n",
    "    for score in confScore0:\n",
    "        if score <= test_score_neg:\n",
    "            rank +=1\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    p_value = rank / len(confScore0)\n",
    "    list_pValue_with_lable0.append((key,p_value))\n",
    "    #print(\"list pValue0\\n\",list_pValue_with_lable0)\n",
    "    return list_pValue_with_lable0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef list_score_test_sample(conformity_scores_zero,key):\\n    seeZero = []\\n    zeroF = []\\n    list_score_test_sample_0 = []\\n    count = 0\\n    for sample, label, score in conformity_scores_zero:\\n        if(count <= 105):\\n            seeZero.append(score)\\n            count += 1\\n        else:\\n            count = 0\\n        #print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score:.3f}\")\\n    zeroF.append(seeZero.copy())\\n    seeZero.clear()\\n    #retrive all measure score for all test sample with assumed label 0\\n    score_test_sample_0 = conformity_scores_zero[-1][2]\\n    list_score_test_sample_0.append((key,score_test_sample_0))\\n    list_score_test_sample_0_array = np.array(list_score_test_sample_0)\\n    return list_score_test_sample_0_array, zeroF\\n    '"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def list_score_test_sample(conformity_scores_zero,key):\n",
    "    seeZero = []\n",
    "    zeroF = []\n",
    "    list_score_test_sample_0 = []\n",
    "    count = 0\n",
    "    for sample, label, score in conformity_scores_zero:\n",
    "        if(count <= 105):\n",
    "            seeZero.append(score)\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "        #print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score:.3f}\")\n",
    "    zeroF.append(seeZero.copy())\n",
    "    seeZero.clear()\n",
    "    #retrive all measure score for all test sample with assumed label 0\n",
    "    score_test_sample_0 = conformity_scores_zero[-1][2]\n",
    "    list_score_test_sample_0.append((key,score_test_sample_0))\n",
    "    list_score_test_sample_0_array = np.array(list_score_test_sample_0)\n",
    "    return list_score_test_sample_0_array, zeroF\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conformity Scores assuming the test sample is +1:\n",
      "Sample: [0 3], Label: 1, Conformity Score: 0.8944271909999159\n",
      "Sample: [2 2], Label: 1, Conformity Score: 1.5811388300841895\n",
      "Sample: [3 3], Label: 1, Conformity Score: 2.5495097567963922\n",
      "Sample: [-1  1], Label: -1, Conformity Score: 1.4142135623730951\n",
      "Sample: [-1 -1], Label: -1, Conformity Score: 0.7071067811865476\n",
      "Sample: [0 1], Label: -1, Conformity Score: 1.0\n",
      "Sample: [0 0], Label: 1, Conformity Score: 0.35355339059327373\n",
      "\n",
      "Conformity Scores assuming the test sample is -1:\n",
      "Sample: [0 3], Label: 1, Conformity Score: 0.8944271909999159\n",
      "Sample: [2 2], Label: 1, Conformity Score: 1.5811388300841895\n",
      "Sample: [3 3], Label: 1, Conformity Score: 2.5495097567963922\n",
      "Sample: [-1  1], Label: -1, Conformity Score: 2.23606797749979\n",
      "Sample: [-1 -1], Label: -1, Conformity Score: 2.91547594742265\n",
      "Sample: [0 1], Label: -1, Conformity Score: 2.0\n",
      "Sample: [0 0], Label: -1, Conformity Score: 2.8284271247461903\n",
      "0.35355339059327373\n",
      "2.8284271247461903\n",
      "p_vlaure for assumed +1 0.143\n",
      "p_vlaure for assumed -1 0.857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "def compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label, epsilon=1e-8):\n",
    "    results = []\n",
    "\n",
    "    # Add the test sample with assumed label to the training set temporarily\n",
    "    extended_samples = np.vstack([train_samples, test_sample])\n",
    "    extended_labels = np.append(train_labels, assumed_label)\n",
    "\n",
    "    # Compute conformity scores for each sample in the extended training set\n",
    "    for i, sample in enumerate(extended_samples):\n",
    "        same_class_dist = float('inf')\n",
    "        diff_class_dist = float('inf')\n",
    "\n",
    "        for j, other_sample in enumerate(extended_samples):\n",
    "            if i == j:  # Skip comparison with itself\n",
    "                continue\n",
    "            \n",
    "            dist = euclidean_distance(sample, other_sample)\n",
    "\n",
    "            if extended_labels[j] == extended_labels[i]:  # Same class\n",
    "                same_class_dist = min(same_class_dist, dist)\n",
    "            else:  # Different class\n",
    "                diff_class_dist = min(diff_class_dist, dist)\n",
    "\n",
    "        # Handle division by zero\n",
    "        if same_class_dist == 0.0:\n",
    "            same_class_dist += epsilon\n",
    "        \n",
    "        conformity = diff_class_dist / same_class_dist\n",
    "        results.append((sample, extended_labels[i], conformity))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "#retrive the conformity score of assumed label +1\n",
    "def retrieve_conformity_scorePos(train_samples, train_labels, test_sample):\n",
    "    # Compute conformity scores assuming label +1\n",
    "    conformity_scores_pos = compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label=1)\n",
    "    test_score_pos = conformity_scores_pos[-1][2]  # Extract conformity score of the test sample\n",
    "    #print(f\"Conformity Score assuming label +1: {test_score_pos:.3f}\")\n",
    "    return test_score_pos\n",
    "\n",
    "#retrive the conformity score of assumed label -1   \n",
    "def retrieve_conformity_scoreNeg(train_samples, train_labels, test_sample):\n",
    "    # Compute conformity scores assuming label -1\n",
    "    conformity_scores_neg = compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label=-1)\n",
    "    test_score_neg = conformity_scores_neg[-1][2]  # Extract conformity score of the test sample\n",
    "    #print(f\"Conformity Score assuming label -1: {test_score_neg:.3f}\")\n",
    "    return test_score_neg\n",
    "\n",
    "\n",
    "#calulate pValue for assumed -1\n",
    "# Define training data\n",
    "train_samples = np.array([[0, 3], [2, 2], [3, 3], [-1, 1], [-1, -1], [0, 1]])\n",
    "train_labels = np.array([1, 1, 1, -1, -1, -1])  # 1 = positive, -1 = negative\n",
    "#train_samples = X_train\n",
    "#train_labels = y_train\n",
    "# Define test sample\n",
    "test_sample = np.array([0, 0])\n",
    "\n",
    "# Compute conformity scores assuming label +1\n",
    "print(\"Conformity Scores assuming the test sample is +1:\")\n",
    "conformity_scores_zero = compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label=1)\n",
    "for sample, label, score in conformity_scores_zero:\n",
    "    print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score}\")\n",
    "\n",
    "# Compute conformity scores assuming label -1\n",
    "print(\"\\nConformity Scores assuming the test sample is -1:\")\n",
    "conformity_scores_neg = compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label=-1)\n",
    "for sample, label, score in conformity_scores_neg:\n",
    "    print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score}\")\n",
    "\n",
    "\n",
    "\n",
    "confScorePos = retrieve_conformity_scorePos(train_samples, train_labels, test_sample)\n",
    "print(confScorePos)\n",
    "confScoreNeg = retrieve_conformity_scoreNeg(train_samples, train_labels, test_sample)\n",
    "print(f\"{confScoreNeg}\")\n",
    "\n",
    "\n",
    "#calulate pValue for assumed +1\n",
    "def pValuePos(test_score_pos, conformity_scores_pos):\n",
    "    rank = 0\n",
    "    for sample, label, score in conformity_scores_pos:\n",
    "        if score <= test_score_pos:\n",
    "            rank +=1\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    p_value = rank / len(conformity_scores_pos) \n",
    "    \n",
    "    return p_value \n",
    "\n",
    "\n",
    "#calulate pValue for assumed -1\n",
    "def pValueNeg(test_score_neg, conformity_scores_neg):\n",
    "    rank = 0\n",
    "    for sample, label, score in conformity_scores_neg:\n",
    "        if score <= test_score_neg:\n",
    "            rank +=1\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    p_value = rank / len(conformity_scores_neg) \n",
    "    #print(len(conformity_scores_neg))\n",
    "    return p_value \n",
    "\n",
    "\n",
    "#calulate pValue for assumed +1\n",
    "pValueP = pValuePos(confScorePos, conformity_scores_zero)\n",
    "print(f\"p_vlaure for assumed +1 {pValueP:.3f}\")\n",
    "\n",
    "#calulate pValue for assumed -1\n",
    "pValueN = pValueNeg(confScoreNeg, conformity_scores_neg)\n",
    "print(f\"p_vlaure for assumed -1 {pValueN:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conformity Scores assuming the test sample is +1:\n",
    "Sample: [0 3], Label: 1, Conformity Score: 0.8944271909999159\n",
    "Sample: [2 2], Label: 1, Conformity Score: 1.5811388300841895\n",
    "Sample: [3 3], Label: 1, Conformity Score: 2.5495097567963922\n",
    "Sample: [-1  1], Label: -1, Conformity Score: 1.4142135623730951\n",
    "Sample: [-1 -1], Label: -1, Conformity Score: 0.7071067811865476\n",
    "Sample: [0 1], Label: -1, Conformity Score: 1.0\n",
    "Sample: [0 0], Label: 1, Conformity Score: 0.35355339059327373\n",
    "\n",
    "Conformity Scores assuming the test sample is -1:\n",
    "Sample: [0 3], Label: 1, Conformity Score: 0.8944271909999159\n",
    "Sample: [2 2], Label: 1, Conformity Score: 1.5811388300841895\n",
    "Sample: [3 3], Label: 1, Conformity Score: 2.5495097567963922\n",
    "Sample: [-1  1], Label: -1, Conformity Score: 2.23606797749979\n",
    "Sample: [-1 -1], Label: -1, Conformity Score: 2.91547594742265\n",
    "Sample: [0 1], Label: -1, Conformity Score: 2.0\n",
    "Sample: [0 0], Label: -1, Conformity Score: 2.8284271247461903\n",
    "0.35355339059327373\n",
    "2.8284271247461903\n",
    "p_vlaure for assumed +1 0.143\n",
    "p_vlaure for assumed -1 0.857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert_array\n",
      " [[[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.05357142857142857 ]]\n",
      "\n",
      " [[2.                   0.017857142857142856]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.1875              ]]\n",
      "\n",
      " [[0.                   0.8214285714285714  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.9196428571428571  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.5178571428571429  ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.7857142857142857  ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.39285714285714285 ]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.7678571428571429  ]]\n",
      "\n",
      " [[0.                   0.9107142857142857  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.044642857142857144]]\n",
      "\n",
      " [[2.                   0.044642857142857144]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.14285714285714285 ]]\n",
      "\n",
      " [[0.                   0.9821428571428571  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.9285714285714286  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.8035714285714286  ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.10714285714285714 ]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.44642857142857145 ]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.8392857142857143  ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.625               ]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.1875              ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.9017857142857143  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.7946428571428571  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.5803571428571429  ]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.26785714285714285 ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.044642857142857144]]\n",
      "\n",
      " [[2.                   0.044642857142857144]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.11607142857142858 ]]\n",
      "\n",
      " [[0.                   0.7589285714285714  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.5714285714285714  ]]\n",
      "\n",
      " [[0.                   0.6607142857142857  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.13392857142857142 ]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.5                 ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.9196428571428571  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.1875              ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.6160714285714286  ]]\n",
      "\n",
      " [[0.                   0.9196428571428571  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.1875              ]]\n",
      "\n",
      " [[0.                   0.008928571428571428]]\n",
      "\n",
      " [[1.                   0.08035714285714286 ]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.7946428571428571  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]\n",
      "\n",
      " [[0.                   0.9017857142857143  ]]\n",
      "\n",
      " [[1.                   0.008928571428571428]]\n",
      "\n",
      " [[2.                   0.008928571428571428]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"list pValue0\\n\",len(list_pValue0))\\nprint(\"list pValue1\\n\",list_pValue1)\\nprint(\"list pValue2\\n\",list_pValue2)\\ntot_list_pValues.append(list_pValue0)\\ntot_list_pValues.append(list_pValue1)\\ntot_list_pValues.append(list_pValue2)\\nprint(\"total list pValues\\n\",tot_list_pValues)\\n\\n\\n\\n#calulate pValue for assumed 1\\nfor i in range(len(list_score_test_sample_0_array)):\\n    pValueP = pValueZero(list_score_test_sample_0_array[i], zeroF[i])\\n    print(f\"p_vlaure for assumed 0 and all iteration \\n {pValueP} {i}\")\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "'''\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "def compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label, epsilon=1e-8):\n",
    "    results = []\n",
    "\n",
    "    # Add the test sample with assumed label to the training set temporarily\n",
    "    extended_samples = np.vstack([train_samples, test_sample])\n",
    "    extended_labels = np.append(train_labels, assumed_label)\n",
    "\n",
    "    # Compute conformity scores for each sample in the extended training set\n",
    "    for i, sample in enumerate(extended_samples):\n",
    "        same_class_dist = float('inf')\n",
    "        diff_class_dist = float('inf')\n",
    "\n",
    "        for j, other_sample in enumerate(extended_samples):\n",
    "            if i == j:  # Skip comparison with itself\n",
    "                continue\n",
    "            \n",
    "            dist = distance(sample, other_sample)\n",
    "\n",
    "            if extended_labels[j] == extended_labels[i]:  # Same class\n",
    "                same_class_dist = min(same_class_dist, dist)\n",
    "            else:  # Different class\n",
    "                diff_class_dist = min(diff_class_dist, dist)\n",
    "\n",
    "        # Handle division by zero\n",
    "        if same_class_dist == 0.0:\n",
    "            same_class_dist += epsilon\n",
    "        \n",
    "        conformity = diff_class_dist / same_class_dist\n",
    "        results.append((sample, extended_labels[i], conformity))\n",
    "    \n",
    "    return results\n",
    "'''\n",
    "# Define training data\n",
    "#train_samples = np.array([[0, 3], [2, 2], [3, 3], [-1, 1], [-1, -1], [0, 1]])\n",
    "#train_labels = np.array([1, 1, 1, -1, -1, -1])  # 1 = positive, -1 = negative\n",
    "list_score_test_sample_0 = []\n",
    "list_score_test_sample_1 = []\n",
    "list_score_test_sample_2 = []\n",
    "#count = 0\n",
    "seeZero = []\n",
    "zeroF = []\n",
    "seeOne = []\n",
    "oneF = []\n",
    "seeTwo = []\n",
    "twoF = []\n",
    "list_pValue0 = []\n",
    "list_pValue1 = []\n",
    "list_pValue2 = []\n",
    "tot_list_pValues = []\n",
    "tot_list_pValues2 = []\n",
    "for i in X_test: \n",
    "    train_samples = X_train\n",
    "    train_labels = y_train\n",
    "    #print(\"y_train \",len(y_train))\n",
    "    # Define test sample\n",
    "    #test_sample = np.array([0, 0])\n",
    "\n",
    "    test_sample = i\n",
    "\n",
    "    # Compute conformity scores assuming label 0    \n",
    "    #print(\"\\nConformity Scores assuming the test sample is 0:\")\n",
    "    conformity_scores_zero = compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label=0)\n",
    " \n",
    "    count = 0\n",
    "    for sample, label, score in conformity_scores_zero:\n",
    "        if(count <= 105):\n",
    "            seeZero.append(score)\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "        #print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score:.3f}\")\n",
    "    zeroF.append(seeZero.copy())\n",
    "    seeZero.clear()\n",
    "    #retrive all measure score for all test sample with assumed label 0\n",
    "    score_test_sample_0 = conformity_scores_zero[-1][2]\n",
    "    list_score_test_sample_0.append((0,score_test_sample_0))\n",
    "    list_score_test_sample_0_array = np.array(list_score_test_sample_0)\n",
    "\n",
    "\n",
    "    # Compute conformity scores assuming label 1\n",
    "    #print(\"\\nConformity Scores assuming the test sample is 1:\")\n",
    "    conformity_scores_one = compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label=1)\n",
    "\n",
    "    count = 0\n",
    "    for sample, label, score in conformity_scores_one:\n",
    "        if(count <= 105):\n",
    "            seeOne.append(score)\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "        #print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score:.3f}\")\n",
    "    oneF.append(seeOne.copy())\n",
    "    seeOne.clear()\n",
    "    #print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score:.3f}\")\n",
    "    #retrive all measure score for all test sample with assumed label 1\n",
    "    score_test_sample_1 = conformity_scores_one[-1][2]\n",
    "    list_score_test_sample_1.append((1,score_test_sample_1))\n",
    "    list_score_test_sample_1_array = np.array(list_score_test_sample_1)\n",
    "\n",
    "    # Compute conformity scores assuming label 2\n",
    "    #print(\"\\nConformity Scores assuming the test sample is 2:\")\n",
    "    conformity_scores_two = compute_conformity_scores(train_samples, train_labels, test_sample, assumed_label=2)\n",
    "    count = 0\n",
    "    for sample, label, score in conformity_scores_two:\n",
    "        if(count <= 105):\n",
    "            seeTwo.append(score)\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "        #print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score:.3f}\")\n",
    "    twoF.append(seeTwo.copy())\n",
    "    seeTwo.clear()\n",
    "    #print(f\"Sample: {sample}, Label: {label}, Conformity Score: {score:.3f}\")\n",
    "    \n",
    "     #retrive all measure score for all test sample with assumed label 2\n",
    "    score_test_sample_2 = conformity_scores_two[-1][2]\n",
    "    list_score_test_sample_2.append((2,score_test_sample_2))\n",
    "    list_score_test_sample_2_array = np.array(list_score_test_sample_2)\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=40)  # Suppress scientific notation, 5 decimal places\n",
    "#print(f\"list assumed label 0\\n{list_score_test_sample_0_array}\")\n",
    "#print(f\"list assumed label 1\\n{list_score_test_sample_1_array}\")\n",
    "#print(f\"list assumed label 2\\n{list_score_test_sample_2_array}\")\n",
    "\n",
    "\n",
    "'''\n",
    "#calulate pValue for assumed 0\n",
    "def pValueZero(test_score_neg, confScore0):\n",
    "    list_pValue_with_lable0  = []\n",
    "\n",
    "    rank = 0\n",
    "    for score in confScore0:\n",
    "        if score <= test_score_neg:\n",
    "            rank +=1\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    p_value = rank / len(confScore0)\n",
    "    list_pValue_with_lable0.append((0,p_value))\n",
    "    #print(\"list pValue0\\n\",list_pValue_with_lable0)\n",
    "    return list_pValue_with_lable0 \n",
    "\n",
    "#calulate pValue for assumed +1\n",
    "def pValueOne(test_score_neg, confScore1):\n",
    "    list_pValue_with_lable1  = []\n",
    "\n",
    "    rank = 0\n",
    "    for score in confScore1:\n",
    "        if score <= test_score_neg:\n",
    "            rank +=1\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    p_value = rank / len(confScore1)\n",
    "    list_pValue_with_lable1.append((1,p_value))\n",
    "    #print(len(confScore1))\n",
    "    return list_pValue_with_lable1 \n",
    "\n",
    "#calulate pValue for assumed +2\n",
    "def pValueTwo(test_score_neg, confScore2):\n",
    "    list_pValue_with_lable2  = []\n",
    "\n",
    "    rank = 0\n",
    "    for score in confScore2:\n",
    "        if score <= test_score_neg:\n",
    "            rank +=1\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    p_value = rank / len(confScore2)\n",
    "    list_pValue_with_lable2.append((2,p_value))\n",
    "    #print(\"list pValue0\\n\",list_pValue_with_lable2)\n",
    "    return list_pValue_with_lable2 \n",
    "'''\n",
    "#calulate pValue for assumed 0\n",
    "for i in range(len(list_score_test_sample_0_array)):\n",
    "    test_score_0 = list_score_test_sample_0_array[i][1]  # Extract the score from (label, score)\n",
    "    #print(\"ciao \",test_score_0)\n",
    "    conformity_score0 = zeroF[i]  # Get the corresponding list of scores from zeroF\n",
    "    \n",
    "    pValue0 = pValue(test_score_0, conformity_score0,key=0)\n",
    "    list_pValue0.append(pValue0)\n",
    "    #print(f\"p-value for assumed 0 at iteration {pValue0}\")\n",
    "\n",
    "#calulate pValue for assumed 1\n",
    "for i in range(len(list_score_test_sample_0_array)):\n",
    "    test_score_One = list_score_test_sample_1_array[i][1]  # Extract the score from (label, score)\n",
    "    #print(\"ciao \",test_score_One)\n",
    "    conformity_score1 = oneF[i]  # Get the corresponding list of scores from zeroF\n",
    "    \n",
    "    pValue1 = pValue(test_score_One, conformity_score1,key=1)\n",
    "    list_pValue1.append(pValue1)\n",
    "\n",
    "    #print(f\"p-value for assumed 1 at iteration {pValue1}\")\n",
    "\n",
    "#calulate pValue for assumed 2\n",
    "for i in range(len(list_score_test_sample_2_array)):\n",
    "    test_score_Two = list_score_test_sample_2_array[i][1]  # Extract the score from (label, score)\n",
    "    #print(\"ciao \",test_score_Two)\n",
    "    conformity_score2 = twoF[i]  # Get the corresponding list of scores from zeroF\n",
    "    \n",
    "    pValue2 = pValue(test_score_Two, conformity_score2,key=2)\n",
    "    list_pValue2.append(pValue2)\n",
    "\n",
    "    #print(f\"p-value for assumed 2 at iteration {pValue2}\")\n",
    "\n",
    "for i in range(len(list_pValue0)):\n",
    "        tot_list_pValues2.append(list_pValue0[i])\n",
    "        tot_list_pValues2.append(list_pValue1[i])\n",
    "        tot_list_pValues2.append(list_pValue2[i])\n",
    "\n",
    "convert_array = np.array(tot_list_pValues2)\n",
    "\n",
    "print(\"convert_array\\n\", convert_array)\n",
    "'''\n",
    "print(\"list pValue0\\n\",len(list_pValue0))\n",
    "print(\"list pValue1\\n\",list_pValue1)\n",
    "print(\"list pValue2\\n\",list_pValue2)\n",
    "tot_list_pValues.append(list_pValue0)\n",
    "tot_list_pValues.append(list_pValue1)\n",
    "tot_list_pValues.append(list_pValue2)\n",
    "print(\"total list pValues\\n\",tot_list_pValues)\n",
    "\n",
    "\n",
    "\n",
    "#calulate pValue for assumed 1\n",
    "for i in range(len(list_score_test_sample_0_array)):\n",
    "    pValueP = pValueZero(list_score_test_sample_0_array[i], zeroF[i])\n",
    "    print(f\"p_vlaure for assumed 0 and all iteration \\n {pValueP} {i}\")\n",
    "'''\n",
    "#print(f\"sees \\n {zeroF[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average false p_value  0.012335526315789479\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for i in range(len(list_pValue0)):\n",
    "        #print(list_pValue0[i][0][0])\n",
    "        #print(list_pValue0[i][0][1])        \n",
    "'''\n",
    "sum = 0\n",
    "count = 0\n",
    "for i in  range(len(y_test)):\n",
    "        #print(label,list_pValue0[i][0][0])\n",
    "        if(y_test[i] == 0):\n",
    "                #print(\"first \",list_pValue0[i][0][0],y_test[i])\n",
    "                sum += list_pValue1[i][0][1] + list_pValue2[i][0][1] \n",
    "                count += 2\n",
    "                #print(\"counter first\", count)\n",
    "        elif(y_test[i]== 1):\n",
    "                #print(\"second \",list_pValue1[i][0][0],y_test[i])\n",
    "                sum += list_pValue0[i][0][1] + list_pValue2[i][0][1]\n",
    "                count += 2\n",
    "                #print(\"counter secornd\", count)\n",
    "        elif(y_test[i] == 2):\n",
    "                #print(\"third \",list_pValue2[i][0][0],y_test[i])\n",
    "                sum += list_pValue0[i][0][1] + list_pValue1[i][0][1]\n",
    "                count += 2\n",
    "                #print(\"counter third\", count)\n",
    "\n",
    "#print(sum)         \n",
    "#print(\"count \",count)\n",
    "tot = sum /count\n",
    "print(\"average false p_value \", tot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsum_p_value = 0\\ncount = 0\\n\\nfor i in range(len(y_test)):\\n    if y_test[i] == 0:\\n        # Sum the p_values from list_pValue1 and list_pValue2\\n        sum_p_value += list_pValue1[i][0][1] + list_pValue2[i][0][1]\\n        count += 2  # Increment count for two values\\n    elif y_test[i] == 1:\\n        # Sum the p_values from list_pValue0 and list_pValue2\\n        sum_p_value += list_pValue0[i][0][1] + list_pValue2[i][0][1]\\n        count += 2\\n    elif y_test[i] == 2:\\n        # Sum the p_values from list_pValue0 and list_pValue1\\n        sum_p_value += list_pValue0[i][0][1] + list_pValue1[i][0][1]\\n        count += 2\\n\\n# Calculate and print the average p_value\\nif count > 0:\\n    average_p_value = sum_p_value / count\\n    print(\"Average false p_value:\", average_p_value)\\nelse:\\n    print(\"No values to calculate.\")\\n\\nprint(\"Total p_value sum:\", sum_p_value)\\nprint(\"Count of p_values included:\", count)\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "sum_p_value = 0\n",
    "count = 0\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 0:\n",
    "        # Sum the p_values from list_pValue1 and list_pValue2\n",
    "        sum_p_value += list_pValue1[i][0][1] + list_pValue2[i][0][1]\n",
    "        count += 2  # Increment count for two values\n",
    "    elif y_test[i] == 1:\n",
    "        # Sum the p_values from list_pValue0 and list_pValue2\n",
    "        sum_p_value += list_pValue0[i][0][1] + list_pValue2[i][0][1]\n",
    "        count += 2\n",
    "    elif y_test[i] == 2:\n",
    "        # Sum the p_values from list_pValue0 and list_pValue1\n",
    "        sum_p_value += list_pValue0[i][0][1] + list_pValue1[i][0][1]\n",
    "        count += 2\n",
    "\n",
    "# Calculate and print the average p_value\n",
    "if count > 0:\n",
    "    average_p_value = sum_p_value / count\n",
    "    print(\"Average false p_value:\", average_p_value)\n",
    "else:\n",
    "    print(\"No values to calculate.\")\n",
    "\n",
    "print(\"Total p_value sum:\", sum_p_value)\n",
    "print(\"Count of p_values included:\", count)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
